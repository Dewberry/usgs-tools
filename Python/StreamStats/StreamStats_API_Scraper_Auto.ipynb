{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# StreamStats API Scraper Automatic\n",
    "\n",
    "__Description__: Tool to automatically run the [USGS StreamStats tool](https://www.usgs.gov/mission-areas/water-resources/science/streamstats-streamflow-statistics-and-spatial-analysis-tools?qt-science_center_objects=0#qt-science_center_objects) for multiple points within a catchment and return the flow frequency curves and subcatchment boundaries.\n",
    "\n",
    "__Input__: A shapefile containing the latitude and longitude of points on the stream grid for the specified state (confluence and main stem locations).\n",
    "\n",
    "__Output__: GeoJSON file containing the delinated catchment boundary and flow frequency data for each point, as well as a CSV file containing the flow frequency data.\n",
    "\n",
    "---\n",
    "## Load Libraries and Python Options:\n",
    "### Load Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../Core')\n",
    "from StreamStats_API_Scraper import*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the state abbreviation and location of the json file: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = 'NY'  # The state abbreviation in uppercase\n",
    "\n",
    "root_dir = pl.Path(r'C:\\Users\\sputnam\\Desktop\\RI_FEMA_II\\River')\n",
    "inputs_dir = root_dir/'Inputs'\n",
    "outputs_dir = root_dir/'Outputs'\n",
    "\n",
    "riv_gauges = 'riv_gauges_parsed.json'  # Specify the location of the shapefile/json containing the lat/lon of points on the stream grid\n",
    "\n",
    "## Options:\n",
    "use_epsg = '4326'    # Specify a consistent coordinate reference system\n",
    "get_flow = False     # If state is WI set to False\n",
    "print_status = True  # Option to show print statements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the json:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(inputs_dir/riv_gauges) as f:\n",
    "    dic = json.load(f)\n",
    "    \n",
    "print('{0} gages'.format(len(dic['features'])))    \n",
    "dic['features'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select for state of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select = {}\n",
    "for feature in dic['features']:\n",
    "    if feature['properties']['State'] == state:\n",
    "        select[feature['properties']['Gauge_No']] = feature['geometry']['coordinates']\n",
    "print('{0} gages for {1}'.format(len(list(select.keys())), state))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save the state of interest to a geojson file (optional):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coord_df = pd.DataFrame(data={'ID_Num': [k for k in select.keys()], 'Lon':[v[0] for v in select.values()],'Lat':[v[1] for v in select.values()]})\n",
    "coord_df['geometry'] = list(zip(coord_df['Lon'], coord_df['Lat']))\n",
    "coord_df['geometry'] = coord_df['geometry'].apply(Point)\n",
    "gdf = gpd.GeoDataFrame(coord_df, geometry='geometry', crs={'init': 'epsg:%s' %use_epsg})\n",
    "with open(outputs_dir/'{0}.geojson'.format(state), 'w') as f:\n",
    "    geojson.dump(gdf, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Run the API tool for each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polyg = {}   # Dictionary to store the catchment polygons (catchment boundaries) \n",
    "ffdata = {}  # Dictionary to store the outlet flow frequency data dictionaries\n",
    "errors = []\n",
    "\n",
    "for ID_Num, geom in select.items():\n",
    "    try:\n",
    "        lon, lat = geom[0], geom[1]  #  Longitude and latitude for each shapely point and the confluence number\n",
    "        if print_status: \n",
    "            print(\"Lat/Lon/Confluence:\", lat, lon, ID_Num)\n",
    "        polyg[ID_Num], ff_json  = SS_scrape(state, lon, lat, use_epsg, print_status) \n",
    "        if get_flow: \n",
    "            ffdata[ID_Num] = get_peaks(ff_json) \n",
    "            polyg[ID_Num]['features'][0]['ffcurve']  = ffdata[ID_Num]\n",
    "        with open(os.path.join(allresults,'StreamStats_Polygons_{0}.geojson'.format(str(ID_Num))), 'w') as f:\n",
    "            dump(polyg[ID_Num], f)\n",
    "    except:\n",
    "        print('could not process data {}'.format(ID_Num))\n",
    "        errors.append(ID_Num) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Load the results (NOT CHECKED 12-2-19):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files(allresults)\n",
    "gdf2, ffdic=load_results(files, use_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.drop(gdf.loc[gdf['Id'][gdf2['ID_Num']]].index, inplace=True)\n",
    "gdf.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The flow frequency data as a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run this function to construct the summary table for all outlet locations\n",
    "if get_flow: ff_df=ff_summary(ffdic) \n",
    "    \n",
    "#Save the results as a csv\n",
    "if get_flow: ff_df.to_csv(os.path.join(path,'StreamStats_FlowFrequency.csv')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The catchment polygons as a Shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Export the geodataframe as a shapefile\n",
    "gdf2 = convert_attr(gdf2)\n",
    "gdf2.to_file(filename = os.path.join(path,'StreamStats_Polygons.shp')) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The catchment polygons as a geojson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'StreamStats_Polygons.geojson'), 'w') as f:\n",
    "     dump(gdf2, f)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
