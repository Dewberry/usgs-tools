{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## StreamStats API Scraper Automatic\n",
    "\n",
    "__Description__: Tool to automatically run the [USGS StreamStats tool](https://www.usgs.gov/mission-areas/water-resources/science/streamstats-streamflow-statistics-and-spatial-analysis-tools?qt-science_center_objects=0#qt-science_center_objects) for multiple points within a catchment and return the flow frequency curves and subcatchment boundaries.\n",
    "\n",
    "__Input__: A shapefile containing the latitude and longitude of points on the stream grid for the specified state (confluence and main stem locations).\n",
    "\n",
    "__Output__: GeoJSON file containing the delinated catchment boundary and flow frequency data for each point, as well as a CSV file containing the flow frequency data.\n",
    "\n",
    "*Authors*: sputnam@Dewberry.com & slawler@Dewberry.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and Python options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import sys\n",
    "import json\n",
    "sys.path.append(r'../Core')\n",
    "from StreamStats_API_Scraper import *\n",
    "import geopandas as gpd\n",
    "from geojson import dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the state abbreviation and location of the shapefile: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Specify:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The state abbreviation in uppercase\n",
    "state = 'NY' \n",
    "\n",
    "# Specify the location of the shapefile containing the lat/lon of points on\n",
    "# the stream grid\n",
    "path = r'P:\\02\\NY\\R2_BLE_Discovery\\TECHNICAL\\Tioga\\HYDROLOGY\\streamstats' \n",
    "name = 'Confluences.shp' #The name of the shapefile\n",
    "id_field = 'ID_Num'\n",
    "\n",
    "# Specify a consistent coordinate reference system\n",
    "use_epsg = '26918' # '4326' \n",
    "\n",
    "# Location to save the StreamStats results for each polygon\n",
    "allresults=os.path.join(path,'AllStreamStats') \n",
    "if os.path.isdir(allresults) == False: \n",
    "    os.mkdir(allresults)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load the shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the shapefile as a geopandas dataframe\n",
    "# Transform the coordinate reference system of the geodataframe\n",
    "gdf=gpd.read_file(os.path.join(path, name))\n",
    "gdf=gdf.to_crs({'init': 'epsg:{0}'.format(use_epsg)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.index = gdf[id_field].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Initialize parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store the catchment polygons (catchment boundaries)\n",
    "polyg={}\n",
    "\n",
    "# Dictionary to store the outlet flow frequency data dictionaries\n",
    "ffdata={}\n",
    "\n",
    "get_flow=True\n",
    "print_status=True\n",
    "if state=='WI': get_flow=False\n",
    "errors = [0]\n",
    "count = {i:0 for i in gdf[id_field]}\n",
    "keys = count.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the API tool for each point:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lat/Lon/Confluence: 4689364.445002781 274651.35086100595 0.0\n",
      "Line 28: Expecting value: line 1 column 1 (char 0\n",
      "while loop: watershed_data count: 1\n",
      "could not process data 0.0\n",
      "Lat/Lon/Confluence: 4689324.445002781 274621.35086100595 1.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4689394.445002781 274671.35086100595 2.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4689364.445002781 274611.35086100595 3.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4689164.445002781 274531.35086100595 4.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4689064.445002781 274691.35086100595 5.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4689064.445002781 274651.35086100595 6.0\n",
      "Line 28: Expecting value: line 1 column 1 (char 0\n",
      "while loop: watershed_data count: 1\n",
      "could not process data 6.0\n",
      "Lat/Lon/Confluence: 4688884.445002781 274451.35086100595 7.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688854.445002781 274491.35086100595 8.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688644.445002781 274451.35086100595 9.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688644.445002781 274391.35086100595 10.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688534.445002781 274331.35086100595 11.0\n",
      "Line 28: Expecting value: line 1 column 1 (char 0\n",
      "while loop: watershed_data count: 1\n",
      "could not process data 11.0\n",
      "Lat/Lon/Confluence: 4688544.445002781 274291.35086100595 12.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4690704.445002781 275891.35086100595 13.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4690684.445002781 275831.35086100595 14.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688504.445002781 274251.35086100595 15.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688534.445002781 274241.35086100595 16.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4690734.445002781 275621.35086100595 17.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4690684.445002781 275631.35086100595 18.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688404.445002781 273801.35086100595 19.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688464.445002781 273801.35086100595 20.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688354.445002781 274151.35086100595 21.0\n",
      "Fetched Peak Flows\n",
      "could not process data 21.0\n",
      "Lat/Lon/Confluence: 4688304.445002781 274171.35086100595 22.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688004.445002781 273181.35086100595 23.0\n",
      "Line 28: Expecting value: line 1 column 1 (char 0\n",
      "while loop: watershed_data count: 1\n",
      "could not process data 23.0\n",
      "Lat/Lon/Confluence: 4688034.445002781 273181.35086100595 24.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4687734.445002781 273991.35086100595 25.0\n",
      "Line 28: Expecting value: line 1 column 1 (char 0\n",
      "while loop: watershed_data count: 1\n",
      "could not process data 25.0\n",
      "Lat/Lon/Confluence: 4687724.445002781 273951.35086100595 26.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4691084.445002781 275031.35086100595 27.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4691064.445002781 274991.35086100595 28.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688034.445002781 272981.35086100595 29.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4688084.445002781 272981.35086100595 30.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4687564.445002781 273951.35086100595 31.0\n",
      "Fetched Peak Flows\n",
      "Lat/Lon/Confluence: 4687544.445002781 273901.35086100595 32.0\n"
     ]
    }
   ],
   "source": [
    "# while errors != [] and [count[key] > 3 for key in keys]:\n",
    "errors = []\n",
    "for i in gdf.index.values:\n",
    "    try:\n",
    "        # Longitude and latitude for each shapely point and the\n",
    "        # confluence number\n",
    "        lon, lat = gdf.geometry[i].x, gdf.geometry[i].y\n",
    "        ID_Num = gdf[id_field][i]\n",
    "        if print_status: print(\"Lat/Lon/Confluence:\", lat, lon, ID_Num)\n",
    "\n",
    "        # Run the SS_scrape function. Option: set status=False to hide\n",
    "        # print statements\n",
    "        polyg[ID_Num], ff_json = SS_scrape(state, lon, lat,\n",
    "                                           use_epsg, print_status)\n",
    "        if get_flow:\n",
    "            # Use the function above to extract the json data\n",
    "            ffdata[ID_Num] = get_peaks(ff_json)\n",
    "            polyg[ID_Num]['features'][0]['ffcurve'] = ffdata[ID_Num]\n",
    "        \n",
    "        gjson_name = f'StreamStats_Polygons_{int(ID_Num)}.geojson'\n",
    "        with open(os.path.join(allresults, gjson_name), 'w') as f:\n",
    "            dump(polyg[ID_Num], f)\n",
    "    except:\n",
    "        print('could not process data {}'.format(gdf[id_field][i]))\n",
    "        errors.append(gdf[id_field][i])\n",
    "        count[gdf[id_field][i]] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = load_files(allresults)\n",
    "gdf2, ffdic = load_results(files, use_epsg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf.drop(gdf.loc[gdf[id_field][gdf2['ID_Num']]].index, inplace=True)\n",
    "gdf.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Save:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The flow frequency data as a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this function to construct the summary table for all outlet locations\n",
    "if get_flow:\n",
    "    ff_df=ff_summary(ffdic)\n",
    "\n",
    "# Save the results as a csv\n",
    "if get_flow:\n",
    "    ff_df.to_csv(os.path.join(path,'StreamStats_FlowFrequency.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The catchment polygons as a Shapefile:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the geodataframe as a shapefile\n",
    "gdf2 = convert_attr(gdf2)\n",
    "gdf2.to_file(os.path.join(path, 'StreamStats_Polygons.shp'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The catchment polygons as a geojson:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(path,'StreamStats_Polygons.geojson'), 'w') as f:\n",
    "     dump(gdf2, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BLEtools",
   "language": "python",
   "name": "bletools"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
